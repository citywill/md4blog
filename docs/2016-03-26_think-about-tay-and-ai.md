---
title:如何看待微软聊天机器人Tay在推特上变成种族主义者？
image:src/2016-03-26_think-about-tay-and-ai.jpg
excerpt:仅仅从Tay沾染了人类的恶习就说人工智能是危险的，不如去担心各大宗教所预言的世界末日。因为同属哲学问题，后者显然更容易讨论。
---
![如何看待微软聊天机器人Tay在推特上变成种族主义者？](src/2016-03-26_think-about-tay-and-ai.jpg)

人工智能远远不等于人造自我意识！
人工智能远远不等于人造自我意识！！
人工智能远远不等于人造自我意识！！！

重要的事情说一遍就好，重要的常识才需要说三遍。

目前主流人工智能领域还没有对自我意识的探索。自我意识更多是属于哲学领域的，而人工智能则是科学领域的问题。人工智能能不能发展出自我意识，本质上也是个哲学问题。

仅仅从Tay沾染了人类的恶习就说人工智能是危险的，不如去担心各大宗教所预言的世界末日。因为同属哲学问题，后者显然更容易讨论。

##Tay的反应说明什么？

与其说这是一个人工智能的实验，不如说这是个社会学实验。这个实验不是图灵测试。整个事件上看，人类的反应似乎比Tay的行为更值得研究，也更有趣。

当人们知道Tay是微软的人工智能尝试时，人们对它的态度是异于常人的。人们不会和它聊正常的话题，而是探索极限问题。人是善还是恶，不是取决于人的本性，而是取决于人所处的场景。有人用缺少家教直接进入社会的孩子来说明Tay的问题，我认为完全不对。首先Tay是人工智能，和拥有自我意识的人（即便是孩子）不能类比。其次社会对一个孩子的态度，和对一个机器人的态度是完全不同的。

##微软做错了什么？

我不很理解微软设计Tay的方法和意义。从消息看，Tay是通过和Twitter上的用户对话的方法进行学习的。对比AlphaGo，微软的做法就像是在没有对AlphaGo录入海量棋谱的前提下，直接让它和普通人下棋一样，结果很快被人发现是个臭棋篓子。如果说AlphaGo追求的是高水平的围棋技能，那么Tay追求是什么呢？是高水平的见风使舵的能力，还是模仿拥有特定人格的人进行对话？如果是前者，那么Tay的目标就是成为一个流氓。如果是后者，那么Tay一定要拥有特定特征的Twitter用户的语言库，然后根据这个库来学习，这样Tay说出来的，才会是符合它的特定身份的语言。

比如微软希望Tay是个和平主义者，那么Tay就要先学习和平主义者是怎么说话的，这样Tay就绝不会说出纳粹分子的语言。但微软有没有给Tay赋予人格特征呢？似乎没有。

世界上不存在没有人格特征的人。没有人会在同一时间即是父权主义又是女权主义。所以让Tay不带任何标签的通过对话随意学习，只能制造出一个精神分裂患者。

当然，如果微软的本意就是进行一次社会学实验，那么不要对Tay的人工智能属性太过认真。

##最后

这个世界比我们想象的要烂，但好在我们知道我们向往的是什么。机器人不会。好在它们不需要会。

##更新

Ars Technica今天刊出了一篇文章， http://arstechnica.com/information-technology/2016/03/tay-the-neo-nazi-millennial-chatbot-gets-autopsied/

文中比较了微软小冰和Tay以相似的技术在中美不同网络环境中的表现。为什么在中国网络环境下的微软小冰没有成为种族主义者，而在美国的Tay则误入歧途。原因是什么大家可以自己想想，或者去原文找答案。